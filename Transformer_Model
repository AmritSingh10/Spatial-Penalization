# Imports & Config

import os, glob, random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt

# Force CPU for simplicity & to avoid cuda/cpu mismatches
device = torch.device("cpu")

# Path to your patch folders
BASE_DIR = (
    "/dartfs/rc/nosnapshots/V/VaickusL-nb/"
    "EDIT_Students/projects/multimodal/tme_study/colon_st/"
    "dh_colon/patches/"
)

# We'll take the first 30 subfolders (cells)
all_cells = sorted(os.listdir(BASE_DIR))[:30]
print(f"Found {len(all_cells)} cell folders, e.g.: {all_cells[:5]} …")


# Load & Preprocess Patches

# how many patches per cell?
PATCHES_PER_CELL = 40
# target patch size
PATCH_SIZE = (32, 32)

expression_list = []

for cell in all_cells:
    cell_path = os.path.join(BASE_DIR, cell)
    # collect .npy files
    npy_files = sorted(glob.glob(os.path.join(cell_path, "*.npy"))) 
    # sample up to PATCHES_PER_CELL
    npy_files = npy_files[:PATCHES_PER_CELL]
    
    for p in npy_files:
        arr = np.load(p, mmap_mode="r")  # assumed shape (H, W, 3)
        # if needed, resize to 32×32 using simple slicing or skimage; here assume already ~32×32
        if arr.ndim == 3 and arr.shape[2] == 3:
            # normalize to [0,1]
            arr = arr.astype(np.float32) / 255.0
            # if not exactly 32×32, you could downsample via slicing:
            arr = arr[:PATCH_SIZE[0], :PATCH_SIZE[1], :]
            flat = arr.reshape(-1)  # (32*32*3 = 3072,)
            expression_list.append(flat)

print(f"✅ Loaded {len(expression_list)} total patches.")


class PatchDataset(Dataset):
    def __init__(self, vectors):
        self.vectors = vectors

    def __len__(self):
        return len(self.vectors)

    def __getitem__(self, idx):
        x = torch.tensor(self.vectors[idx], dtype=torch.float32)
        return x, x  # auto-encoding

dataset = PatchDataset(expression_list)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)
print(f"Dataset size: {len(dataset)} samples; batches of {dataloader.batch_size}")


class PatchTransformer(nn.Module):
    def __init__(self, input_dim=3072, model_dim=128, nhead=4, depth=3, dropout=0.1):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, model_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim, nhead=nhead, dropout=dropout, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)
        self.output_proj = nn.Linear(model_dim, input_dim)

    def forward(self, x):
        # x: [B, 3072]
        x = self.input_proj(x).unsqueeze(1)   # → [B, 1, D]
        x = self.transformer(x)               # → [B, 1, D]
        x = self.output_proj(x.squeeze(1))    # → [B, 3072]
        return x

model = PatchTransformer().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_fn   = nn.MSELoss()



model.train()
for epoch in range(1, 11):
    total_loss = 0.0
    for x, y in dataloader:
        x = x.to(device)
        y = y.to(device)

        pred = model(x)
        loss = loss_fn(pred, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    avg = total_loss / len(dataloader)
    print(f"Epoch {epoch:2d} | Avg Loss: {avg:.6f}")



model.eval()
with torch.no_grad():
    x, y = dataset[0]                       # [3072], [3072]
    pred = model(x.unsqueeze(0).to(device)) # [1,3072]
    pred = pred.cpu().squeeze(0).numpy()

# true vs. predicted RGB means (first 3 dims)
plt.figure(figsize=(5,4))
plt.plot(y[:3].numpy(), label="True",   lw=2)
plt.plot(pred[:3],       label="Pred", lw=2)
plt.title("RGB Mean: True vs Pred (First Patch)")
plt.xlabel("Channel (R, G, B)")
plt.legend()
plt.show()
